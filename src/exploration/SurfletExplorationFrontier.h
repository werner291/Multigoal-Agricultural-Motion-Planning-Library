// Copyright (c) 2022 University College Roosevelt
//
// All rights reserved.

//
// Created by werner on 30-11-22.
//

#ifndef NEW_PLANNERS_SURFLETEXPLORATIONFRONTIER_H
#define NEW_PLANNERS_SURFLETEXPLORATIONFRONTIER_H

#include <Eigen/Core>
#include <optional>
#include <moveit/robot_state/robot_state.h>
#include <CGAL/Simple_cartesian.h>
#include <CGAL/Kernel/Type_equality_wrapper.h>
#include <CGAL/Search_traits_3.h>
#include <CGAL/Orthogonal_incremental_neighbor_search.h>
#include "SegmentedPointCloud.h"

/**
 * A datastructure that models a robot's knowledge about known and unknown space,
 * centered around some method for determining whether a certain motion enters
 * unknown space and, if so, a position and normal relating to the exploration
 * frontier surface.
 *
 * In principle, the entire volume will initially be considered "unobserved". Repeated
 * calls to `incorporateSnapshot` shall then carve out sections of this volume, changing
 * the points within them from "unobserved" to "observed".
 *
 */
class MotionIntoUnknownSpaceModel {


	/**
	 *
	 *
	 *
	 * @param from_to
	 * @return
	 */
	virtual std::optional<Eigen::Vector3d>
	doesMotionEnterUnknownSpace(const std::pair<const moveit::core::RobotState &, const moveit::core::RobotState &> &from_to) = 0;

	/**
	 *
	 * Report a sensor observation, represented as an eye center point and a point cloud of points observed by that sensor.
	 *
	 * The line segments between the eye point and the observed points will be considered "free",
	 * the rest of the space is untouched.
	 *
	 * @param sensorCenter
	 * @param visible_points
	 *
	 */
	virtual void
	incorporateSnapshot(const Eigen::Vector3d &sensorCenter, const SegmentedPointCloud::ByType &visible_points) = 0;

};

/**
 * A model for an "exploration frontier": an oriented 2D surface in 3D space, where one side is "explored"
 * and the other side is "unexplored".
 *
 * Representation is through an oriented point cloud; that is: a set F of tuples (P,N), called "surflets", where P
 * is the position and N is a normal; N points towards the "explored" side. The implicit surface generated by these
 * surflets is an approximation of the exploration frontier.
 */
class SurfletExplorationFrontier {


	/**
	 *
	 * Incorporate a sensor observation, in the form of an eye center and a set of points visible from that center.
	 *
	 * First, we will generate a set of candidate surflets:
	 *
	 * - For every visible point, create a candidate surflet with normal pointing to the sensor center
	 * -
	 *
	 * @param sensorCenter
	 * @param visible_points
	 */
	void incorporateSnapshot(const Eigen::Vector3d &sensorCenter, const SegmentedPointCloud::ByType &visible_points);

};

/**
 *
 */
class UnionOfVisibleVolumesExplorationFrontier {

	double minTriangleArea;

	/**
	 *
	 * Report a given volume as "observed". The algorithm works as follows:
	 *
	 * We start with a mesh, the interior of which is the "observed volume" O.
	 *
	 * We are given an eye center point as well as a set of observed points. From this, we generate a mesh
	 * volume O' that approximates the volume traversed by the line segments between the sensor center and
	 * the observed points (as well as hypothetical line segments between these).
	 *
	 * Then, we generate the union of O and O', replacing O. (We use CGAL for this, perhaps. See the Boolean operations for NEF polyhedra.)
	 *
	 * As a post-processing step, we collapse any mesh triangles that are excessively small.
	 *
	 * @param sensorCenter 		The eye center point.
	 * @param visible_points 	The sent of points visible from sensorCenter.
	 */
	void incorporateSnapshot(const Eigen::Vector3d &sensorCenter, const SegmentedPointCloud::ByType &visible_points);

};

#endif //NEW_PLANNERS_SURFLETEXPLORATIONFRONTIER_H
